{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import csv\n",
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Scores\n",
    "\n",
    "Use this nb to compute all the pop. scores for all the parties in each nation. The nb uses all the parties in the test sets, plus all the parties ecluded from the training set in the \"parties_to_exclude\" dictionary from nb \"00_generate_bag_of_words.ipynb\".\n",
    "\n",
    "In the __Configuration__ section, fill the \"nations_params\" dictionary with the kind of classifier used for the corresponding nation, the target score used in the Grid Search and the seed for the random number generators. Check \"training_results.json\" for possible values.\n",
    "\n",
    "\n",
    "The ouput data will be saved in .csv format into the \"scores\" folder. \n",
    "\n",
    "__Note__\n",
    "\n",
    "For Spain we will not compute the score for all the regionalist parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations_params ={\n",
    "    \"AT\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"IT\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"FR\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"ES\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"DE\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"NL\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"IT_speeches\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"AUC\",\n",
    "        \"random_state\":1\n",
    "    },\n",
    "    \"IT_manual\":{\n",
    "        \"model\":\"GradientBoosting\",\n",
    "        \"target\": \"F1\",\n",
    "        \"random_state\":15\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parties data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties_to_exclude = {\n",
    "    \"IT\":['Forward Italy', 'PdL', 'Italy of Values', 'Casapound', 'Houses of Freedom'],\n",
    "\n",
    "    \"FR\":['The Greens','French Communist Party', \"Nouveau Parti Anticapitaliste\", \"Resistons\",'Debout la France'],\n",
    "    \"AT\":['Peter Pilz List'],\n",
    "\n",
    "    \"NL\":['DENK','Party for the Animals','Reformed Political Party','50Plus','Green Left'],\n",
    "    \"ES\":['Amaiur','Andalusian Party','Aragonist Council','Basque Country Unite'\\\n",
    "          ,'Basque Nationalist Party','Basque Solidarity','Canarian Coalition','Catalan Republican Left'\\\n",
    "          ,'Citizens','Commitment-Q','Commitment-We can-It is time','Democratic Convergence of Catalonia'\\\n",
    "          ,'Forum Asturias','Future Yes','Galician Nationalist Bloc','In Tide',\"Navarrese People's Union\",'Valencian style'],\n",
    "    \"DE\":['Pirates']\n",
    "}\n",
    "\n",
    "nations = list(nations_params.keys())\n",
    "numbers = [str(n) for n in range(1000)]\n",
    "p_train = 0.7\n",
    "def cut_words(w_list):\n",
    "    return [w for w in w_list if len(w)>2 and w not in numbers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading model for IT_manual...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6c62b13058a2>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  texts = np.array([cut_words(record[\"clean_text\"]) for record in data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing test data scores for IT_manual...\n",
      "saving scores for IT_manual...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nation in nations:\n",
    "        \n",
    "    print(\"reading model for {}...\".format(nation))\n",
    "    model_type, target, random_state = nations_params[nation].values()\n",
    "\n",
    "    params = pickle.load(open(\"./models/{0}_{1}_{2}_{3}_best_model_params.pkl\".format(nation, model_type, target, random_state),'rb'))\n",
    "    model = pickle.load(open(\"./models/{0}_{1}_{2}_{3}_best_model.pkl\".format(nation, model_type, target, random_state),\"rb\"))\n",
    "    indices_test = pickle.load(open(\"./models/{0}_{1}_{2}_{3}_test_indices.pkl\".format(nation, model_type, target, random_state),'rb'))\n",
    "    max_thresh = params[\"threshold\"]\n",
    "\n",
    "    ###########################################\n",
    "    \n",
    "    if nation in [\"IT_speeches\",\"IT_manual\"]:\n",
    "        data = json.load(open(\"./datasets/{}_sentences.json\".format(nation),\"r\"))        \n",
    "    else:\n",
    "        data = json.load(open(\"./datasets/{}_manifesto_sentences.json\".format(nation),\"r\"))\n",
    "\n",
    "    texts = np.array([cut_words(record[\"clean_text\"]) for record in data])\n",
    "    texts = np.array([\" \".join(sent) for sent in texts])\n",
    "    parties = np.array([record[\"party\"] for record in data])\n",
    "    years = np.array([record[\"year\"] for record in data])\n",
    "    orientations = np.array([record[\"orientation\"] for record in data])\n",
    "    indices = np.arange(0,len(texts)).astype(int)\n",
    "    \n",
    "    if nation ==\"IT_speeches\":\n",
    "        to_exclude = np.array([party in parties_to_exclude[\"IT\"] for party in parties])\n",
    "    elif nation == \"IT_manual\":\n",
    "        to_exclude = np.array([False for sent in texts])\n",
    "    else:\n",
    "        to_exclude = np.array([party in parties_to_exclude[nation] for party in parties])\n",
    "\n",
    "    texts_kept = texts[~to_exclude]\n",
    "    parties_kept = parties[~to_exclude]\n",
    "    years_kept = years[~to_exclude]\n",
    "    orientations = orientations[~to_exclude]\n",
    "\n",
    "    texts_train,texts_test, parties_train, parties_test, years_train, years_test = sklearn.model_selection.train_test_split(texts_kept,parties_kept, years_kept,random_state=random_state, test_size=1-p_train)\n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "    X_train = (vectorizer.fit_transform(texts_train)>0).astype(int)\n",
    "    X_test = (vectorizer.transform(texts_test)>0).astype(int)\n",
    "    \n",
    "\n",
    "    ############################################\n",
    "    Y_test = (model.predict_proba(X_test)[:,1]>max_thresh)\n",
    "    \n",
    "    print(\"computing test data scores for {}...\".format(nation))    \n",
    "    global_scores = {}\n",
    "    global_scores_counts = {}\n",
    "    score_in_time = {}\n",
    "    score_in_time_counts = {}\n",
    "\n",
    "    for party in set(parties_test):\n",
    "        iii = np.where(parties_test==party)[0]\n",
    "        global_scores[party] = np.mean(Y_test[iii])\n",
    "        global_scores_counts[party] = len(Y_test[iii])\n",
    "\n",
    "\n",
    "    for party, year in zip(parties_test, years_test):\n",
    "        iii = np.where((parties_test==party) & (years_test==year))[0]\n",
    "        score_in_time[(party, year)] = np.mean(Y_test[iii])\n",
    "        score_in_time_counts[(party, year)] = len(Y_test[iii])\n",
    "    \n",
    "    if to_exclude.astype(int).sum()!=0:\n",
    "        print(\"reading excluded parties scores for {}...\".format(nation)) \n",
    "        texts_excluded = texts[to_exclude]\n",
    "        parties_excluded = parties[to_exclude]\n",
    "        X_excluded = (vectorizer.transform(texts_excluded)>0).astype(int)\n",
    "        Y_excluded = (model.predict_proba(X_excluded)[:,1]>max_thresh)\n",
    "        parties_excluded = parties[to_exclude]\n",
    "        years_excluded = years[to_exclude]\n",
    "\n",
    "        for party in set(parties_excluded):\n",
    "            iii = np.where(parties_excluded==party)[0]\n",
    "            global_scores[party] = np.mean(Y_excluded[iii])\n",
    "            global_scores_counts[party] = len(Y_excluded[iii])\n",
    "\n",
    "        for party, year in zip(parties_excluded, years_excluded):\n",
    "            iii = np.where((parties_excluded==party) & (years_excluded==year))[0]\n",
    "            score_in_time[(party, year)] = np.mean(Y_excluded[iii])\n",
    "            score_in_time_counts[(party, year)] = len(Y_excluded[iii])\n",
    "\n",
    "    ###add party orientation\n",
    "    party_orientation = {}\n",
    "    for record in data:\n",
    "        party = record[\"party\"]\n",
    "        orientation = record[\"orientation\"]\n",
    "        party_orientation[party] = orientation\n",
    "            \n",
    "\n",
    "    print(\"saving scores for {}...\".format(nation))    \n",
    "\n",
    "    global_scores_df = pd.DataFrame({\"party\":global_scores.keys(),\"score\":global_scores.values()})\n",
    "    global_scores_df[\"orientation\"] = [party_orientation[party] for party in global_scores_df.party]\n",
    "    global_scores_df[\"counts\"] = [global_scores_counts[party] for party in global_scores_df.party]\n",
    "\n",
    "    global_scores_df.to_csv(\"./scores/global_scores_{}.csv\".format(nation), index=False)\n",
    "\n",
    "    score_in_time_df = pd.DataFrame({\"party\":[k[0] for k in score_in_time.keys()],\"year\":[k[1] for k in score_in_time.keys()],\"score\":score_in_time.values()})\n",
    "    score_in_time_df[\"orientation\"] = [party_orientation[party] for party in score_in_time_df.party]\n",
    "    score_in_time_df[\"counts\"] = [score_in_time_counts[(party,year)] for party,year in score_in_time_df[[\"party\", \"year\"]].values]\n",
    "\n",
    "    score_in_time_df.to_csv(\"./scores/scores_in_time_{}.csv\".format(nation), index=False)\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
